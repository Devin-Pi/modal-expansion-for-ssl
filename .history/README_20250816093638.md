# Modal Expansion-based Data Generation Approach for Deep Learning-Enabled Sound Source Localization in a Small Enclosure
This repository contains the python implementation for the *dataset generation* part of the paper  "Modal Expansion-based Data Generation Approach for Deep Learning-Enabled Sound Source Localization in a Small Enclosure".
![related work](/asset/sanky.pdf)

## Dataset
- Source signals: [LibriSpeech](https://www.openslr.org/12/)

These datasets mentioned above can be downloaded from this [OneDrive link](https://connectpolyu-my.sharepoint.com/:f:/g/personal/22123553r_connect_polyu_hk/EgHVOLP0P8VMvVoZ5DNWYCkBCUWYkaa93QJGnw-Glx4Qlw?e=Zs8iOB).

The data directory structure is shown as follows:

```
.
|---data
    |---LibriSpeech
        |---dev-clean
        |---test-clean
        |---train-clean-100
    |---test (generated)
    |---train (generated)
    |---dev (generated)
```

## Get Started
### Dependencies
We strongly recommend that you can use [VSCode](https://code.visualstudio.com/) and [Docker](https://www.docker.com/) for this project, it can save you much timeüòÅ! Note that the related configurations has already been within `.devcontainer`. The detail information can be found in this [Tutorial_for_Vscode&Dokcer](https://github.com/Devin-Pi/Tutorial_for_VScode_Docker).

### Configurations
The realted configurations are all saved in `config/`.
- The `data_simu.yaml` is used to configure the data generation.
- The `tcrnn.yaml` is used to configure the dataloader, model training & test.

You can change the value of these items based on your need.

Note: [webrtcvad](https://github.com/wiseman/py-webrtcvad).

### üöÄ Quick Start
- **Data Generation**
Generate the training data:
```zsh
python data_simu.py DATA_SIMU.TRAIN=True DATA_SIMU.TRAIN_NUM=10000
```
In the same way, you can also generate the validation and test datasets by changing the `DATA_SIMU.TRAIN=True` to `DATA_SIMU.DEV=True` or `DATA_SIMU.TEST=True`.
- **Model Training**
```zsh
python main_crnn.py fit --config /workspaces/TCRNN/config/tcrnn.yaml
```
The parameter for `--config` should point to your config file path.
- **Model Evaluation**
1) Change the `ckpt_path` in the `config/tcrnn.yaml` to the trained model weight.
2) Use Multiple GPUs or Single GPU to test the model performance.
```zsh
python main_crnn.py test --config /workspaces/TCRNN/config/tcrnn.yaml
```
If you want to evaluate the model using the Single GPU, you can change the value of the `devices` from `"0,1"` to `"0,"` in the `config/tcrnn.yaml`.

## üéì Citation
If you find our work useful in your research, please consider citing:
```
@article{pi2025uncertainty,
  author={Pi, Rendong and Yu, Xiang},
  journal={IEEE Transactions on Instrumentation and Measurement},
  title={Uncertainty Estimation for Sound Source Localization With Deep Learning},
  year={2025},
  volume={74},
  number={},
  pages={1-12},
  doi={10.1109/TIM.2024.3522632}
}


@inproceedings{pi2024tssl,
  title={TSSL: Trusted Sound Source Localization},
  author={Pi, Rendong and Song, Yang and Li, Linfeng and Yu, Xiang and Cheng, Li},
  booktitle={INTER-NOISE and NOISE-CON Congress and Conference Proceedings},
  volume={270},
  number={11},
  pages={941--949},
  year={2024},
  organization={Institute of Noise Control Engineering}
}
```